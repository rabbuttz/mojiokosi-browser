<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>音声録音・文字起こしアプリ</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            background-color: #f5f7fa;
            color: #333;
        }
        .container {
            background-color: white;
            padding: 2rem;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            max-width: 800px;
            width: 100%;
            position: relative;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            margin-bottom: 2rem;
        }
        .input-group {
            display: flex;
            margin-bottom: 1rem;
        }
        input[type="password"] {
            flex-grow: 1;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px 0 0 5px;
            font-size: 16px;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 5px;
            transition: background-color 0.3s ease;
        }
        button:hover {
            background-color: #2980b9;
        }
        button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        #saveApiKey {
            border-radius: 0 5px 5px 0;
        }
        .button-group {
            display: flex;
            justify-content: space-between;
            margin-top: 1rem;
        }
        #visualizer {
            width: 100%;
            height: 100px;
            background-color: #f0f0f0;
            margin-top: 20px;
            border-radius: 10px;
            overflow: hidden;
        }
        #status {
            margin-top: 20px;
            font-weight: bold;
        }
        #transcription-container {
            position: relative;
        }
        #transcription {
            text-align: left;
            white-space: pre-wrap;
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 10px;
            border: 1px solid #e0e0e0;
            max-height: 300px;
            overflow-y: auto;
            margin-top: 40px; /* マージンを追加してボタンのスペースを確保 */
        }
        #transcription-buttons {
            position: absolute;
            top: 0;
            right: 10px;
        }
        .small-button {
            padding: 5px 10px;
            font-size: 12px;
            margin-left: 5px;
        }
        canvas {
            width: 100% !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>音声録音・文字起こしアプリ</h1>
        <div class="input-group">
            <input type="password" id="apiKey" placeholder="Whisper APIキー">
            <button id="saveApiKey">保存</button>
        </div>
        <div class="button-group">
            <button id="startRecording">録音開始</button>
            <button id="stopRecording" disabled>録音停止</button>
            <button id="sendToApi" disabled>文字起こし開始</button>
        </div>
        <div id="visualizer">
            <canvas id="audioVisualizer"></canvas>
        </div>
        <div id="status"></div>
        <div id="transcription-container">
            <div id="transcription-buttons">
                <button class="small-button" id="saveToFile" disabled>保存</button>
                <button class="small-button" id="copyToClipboard" disabled>コピー</button>
            </div>
            <div id="transcription"></div>
        </div>
    </div>

    <script>
        let mediaRecorder;
        let audioChunks = [];
        let audioContext;
        let analyser;
        let visualizerCanvas;
        let visualizerCanvasCtx;

        const apiKeyInput = document.getElementById('apiKey');
        const saveApiKeyButton = document.getElementById('saveApiKey');
        const startRecordingButton = document.getElementById('startRecording');
        const stopRecordingButton = document.getElementById('stopRecording');
        const sendToApiButton = document.getElementById('sendToApi');
        const statusDiv = document.getElementById('status');
        const transcriptionDiv = document.getElementById('transcription');
        const saveToFileButton = document.getElementById('saveToFile');
        const copyToClipboardButton = document.getElementById('copyToClipboard');

        // APIキーの保存
        saveApiKeyButton.addEventListener('click', () => {
            const apiKey = apiKeyInput.value;
            if (apiKey) {
                localStorage.setItem('whisperApiKey', apiKey);
                statusDiv.textContent = 'APIキーが保存されました';
                apiKeyInput.value = '';
            } else {
                statusDiv.textContent = 'APIキーを入力してください';
            }
        });

        // 保存されたAPIキーの読み込み
        const savedApiKey = localStorage.getItem('whisperApiKey');
        if (savedApiKey) {
            apiKeyInput.value = savedApiKey;
        }

        startRecordingButton.addEventListener('click', startRecording);
        stopRecordingButton.addEventListener('click', stopRecording);
        sendToApiButton.addEventListener('click', sendToApi);
        saveToFileButton.addEventListener('click', saveToFile);
        copyToClipboardButton.addEventListener('click', copyToClipboard);

        // 音声ビジュアライザーの設定
        visualizerCanvas = document.getElementById('audioVisualizer');
        visualizerCanvasCtx = visualizerCanvas.getContext('2d');

        function resizeCanvas() {
            visualizerCanvas.width = visualizerCanvas.offsetWidth;
            visualizerCanvas.height = visualizerCanvas.offsetHeight;
        }

        window.addEventListener('resize', resizeCanvas);
        resizeCanvas();

        function drawVisualizer() {
            requestAnimationFrame(drawVisualizer);

            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            analyser.getByteTimeDomainData(dataArray);

            visualizerCanvasCtx.fillStyle = 'rgb(200, 200, 200)';
            visualizerCanvasCtx.fillRect(0, 0, visualizerCanvas.width, visualizerCanvas.height);

            visualizerCanvasCtx.lineWidth = 2;
            visualizerCanvasCtx.strokeStyle = 'rgb(0, 0, 0)';

            visualizerCanvasCtx.beginPath();

            const sliceWidth = visualizerCanvas.width * 1.0 / bufferLength;
            let x = 0;

            for (let i = 0; i < bufferLength; i++) {
                const v = dataArray[i] / 128.0;
                const y = v * visualizerCanvas.height / 2;

                if (i === 0) {
                    visualizerCanvasCtx.moveTo(x, y);
                } else {
                    visualizerCanvasCtx.lineTo(x, y);
                }

                x += sliceWidth;
            }

            visualizerCanvasCtx.lineTo(visualizerCanvas.width, visualizerCanvas.height / 2);
            visualizerCanvasCtx.stroke();
        }

        async function startRecording() {
            audioChunks = [];
            try {
                console.log("録音開始処理を開始します...");
                statusDiv.textContent = "録音の準備中...";

                console.log("デスクトップ音声のキャプチャを開始します...");
                const desktopStream = await navigator.mediaDevices.getDisplayMedia({
                    video: true,
                    audio: true
                });
                console.log("デスクトップ音声のキャプチャに成功しました。");

                console.log("マイク音声のキャプチャを開始します...");
                const micStream = await navigator.mediaDevices.getUserMedia({
                    audio: true,
                    video: false
                });
                console.log("マイク音声のキャプチャに成功しました。");

                console.log("オーディオコンテキストを設定中...");
                audioContext = new AudioContext();
                analyser = audioContext.createAnalyser();
                const desktopSource = audioContext.createMediaStreamSource(desktopStream);
                const micSource = audioContext.createMediaStreamSource(micStream);
                const destination = audioContext.createMediaStreamDestination();

                desktopSource.connect(analyser);
                micSource.connect(analyser);
                analyser.connect(destination);

                console.log("ビジュアライザーの描画を開始します...");
                drawVisualizer();

                console.log("MediaRecorderを設定中...");
                mediaRecorder = new MediaRecorder(destination.stream);

                mediaRecorder.ondataavailable = event => {
                    console.log("データが利用可能です:", event.data);
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = () => {
                    console.log("録音が停止しました");
                    desktopStream.getTracks().forEach(track => track.stop());
                    micStream.getTracks().forEach(track => track.stop());
                };

                console.log("録音を開始します...");
                mediaRecorder.start(1000); // 1秒ごとにデータを取得
                startRecordingButton.disabled = true;
                stopRecordingButton.disabled = false;
                sendToApiButton.disabled = true;
                statusDiv.textContent = '録音中...';
                console.log("録音が正常に開始されました。");
            } catch (error) {
                console.error('録音の開始に失敗しました:', error);
                statusDiv.textContent = `録音の開始に失敗しました: ${error.message}`;
                startRecordingButton.disabled = false;
                stopRecordingButton.disabled = true;
                sendToApiButton.disabled = true;
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                startRecordingButton.disabled = false;
                stopRecordingButton.disabled = true;
                sendToApiButton.disabled = false;
                statusDiv.textContent = '録音が完了しました。「文字起こし開始」ボタンを押して文字起こしを開始できます。';
            }
        }

        async function sendToApi() {
            try {
                sendToApiButton.disabled = true;
                statusDiv.textContent = '文字起こしを開始します...';
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                console.log("録音されたオーディオデータのサイズ:", audioBlob.size);
                if (audioBlob.size === 0) {
                    throw new Error('録音されたオーディオがありません');
                }
                await processAudio(audioBlob);
            } catch (error) {
                console.error('文字起こしに失敗しました:', error);
                statusDiv.textContent = '文字起こしに失敗しました: ' + error.message;
            } finally {
                sendToApiButton.disabled = false;
            }
        }

        async function processAudio(audioBlob) {
            const chunks = await splitAudioIntoChunks(audioBlob);
            let fullTranscription = '';

            for (let i = 0; i < chunks.length; i++) {
                statusDiv.textContent = `文字起こし中... (${i + 1}/${chunks.length})`;
                const transcription = await transcribeAudio(chunks[i]);
                fullTranscription += transcription + '\n';
                transcriptionDiv.textContent = fullTranscription;
            }

            statusDiv.textContent = '文字起こしが完了しました';
            saveToFileButton.disabled = false;
            copyToClipboardButton.disabled = false;
        }

        async function splitAudioIntoChunks(audioBlob, maxSizeInBytes = 24 * 1024 * 1024) {
            const chunks = [];
            let start = 0;
            const duration = await getAudioDuration(audioBlob);
            console.log(`オーディオの長さ: ${duration} 秒`);
            if (duration === 0 || duration === Infinity) {
                throw new Error('オーディオの長さが無効です');
            }
            const bytesPerSecond = audioBlob.size / duration;
            console.log(`バイト/秒: ${bytesPerSecond}`);

            while (start < duration) {
                const chunkDuration = maxSizeInBytes / bytesPerSecond;
                const end = Math.min(start + chunkDuration, duration);
                console.log(`オーディオチャンク: 開始=${start} 秒, 終了=${end} 秒`);
                const chunk = await sliceAudio(audioBlob, start, end);
                chunks.push(chunk);
                start = end;
            }

            return chunks;
        }

        async function getAudioDuration(audioBlob) {
            return new Promise((resolve, reject) => {
                const audio = new Audio(URL.createObjectURL(audioBlob));
                audio.addEventListener('loadedmetadata', () => {
                    if (audio.duration === Infinity) {
                        // ブラウザがオーディオのメタデータを正しく読み取れなかった場合
                        audio.currentTime = Number.MAX_SAFE_INTEGER;
                        audio.addEventListener('timeupdate', function() {
                            audio.currentTime = 0;
                            resolve(audio.duration);
                        }, { once: true });
                    } else {
                        resolve(audio.duration);
                    }
                });
                audio.addEventListener('error', () => {
                    reject(new Error('オーディオの長さを取得できませんでした'));
                });
            });
        }

        async function sliceAudio(audioBlob, start, end) {
            const audioContext = new AudioContext();
            const arrayBuffer = await audioBlob.arrayBuffer();
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
            console.log(`オーディオサンプルレート: ${audioBuffer.sampleRate}`);
            console.log(`オーディオチャンネル数: ${audioBuffer.numberOfChannels}`);
            console.log(`オーディオサンプル数: ${audioBuffer.length}`);
            
            const sampleRate = audioBuffer.sampleRate;
            const numberOfChannels = audioBuffer.numberOfChannels;
            const startSample = Math.floor(start * sampleRate);
            const endSample = Math.floor(end * sampleRate);
            
            // 無限大の値を避けるための修正
            const validStartSample = Math.max(0, Math.min(startSample, audioBuffer.length));
            const validEndSample = Math.max(validStartSample, Math.min(endSample, audioBuffer.length));

            if (validStartSample >= validEndSample) {
                throw new Error('無効なオーディオサンプル範囲');
            }

            console.log(`オーディオサンプル範囲: 開始=${validStartSample}, 終了=${validEndSample}, 長さ=${validEndSample - validStartSample}`);

            const newAudioBuffer = audioContext.createBuffer(numberOfChannels, validEndSample - validStartSample, sampleRate);

            for (let channel = 0; channel < numberOfChannels; channel++) {
                const channelData = audioBuffer.getChannelData(channel);
                newAudioBuffer.copyToChannel(channelData.slice(validStartSample, validEndSample), channel);
            }

            const newAudioBlob = await audioBufferToWav(newAudioBuffer);
            return new Blob([newAudioBlob], { type: 'audio/wav' });
        }

        async function audioBufferToWav(audioBuffer) {
            const wavEncoder = new WavEncoder(audioBuffer.sampleRate, audioBuffer.numberOfChannels);
            const channelData = [];
            for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
                channelData.push(audioBuffer.getChannelData(i));
            }
            wavEncoder.encode(channelData);
            return wavEncoder.finish();
        }

        class WavEncoder {
            constructor(sampleRate, numChannels) {
                this.sampleRate = sampleRate;
                this.numChannels = numChannels;
                this.numSamples = 0;
                this.dataViews = [];
            }

            encode(channelData) {
                const len = channelData[0].length;
                const buf = new ArrayBuffer(len * this.numChannels * 2);
                const view = new DataView(buf);
                let offset = 0;
                for (let i = 0; i < len; i++) {
                    for (let c = 0; c < this.numChannels; c++) {
                        const sample = channelData[c][i];
                        view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
                        offset += 2;
                    }
                }
                this.dataViews.push(view);
                this.numSamples += len;
            }

            finish() {
                const dataLength = this.numChannels * this.numSamples * 2;
                const buffer = new ArrayBuffer(44 + dataLength);
                const view = new DataView(buffer);

                // Write WAV header
                writeString(view, 0, 'RIFF');
                view.setUint32(4, 36 + dataLength, true);
                writeString(view, 8, 'WAVE');
                writeString(view, 12, 'fmt ');
                view.setUint32(16, 16, true);
                view.setUint16(20, 1, true);
                view.setUint16(22, this.numChannels, true);
                view.setUint32(24, this.sampleRate, true);
                view.setUint32(28, this.sampleRate * this.numChannels * 2, true);
                view.setUint16(32, this.numChannels * 2, true);
                view.setUint16(34, 16, true);
                writeString(view, 36, 'data');
                view.setUint32(40, dataLength, true);

                // Write audio data
                let offset = 44;
                for (let i = 0; i < this.dataViews.length; i++) {
                    const dataView = this.dataViews[i];
                    for (let j = 0; j < dataView.byteLength; j++) {
                        view.setUint8(offset, dataView.getUint8(j));
                        offset++;
                    }
                }

                return buffer;
            }
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        async function transcribeAudio(audioBlob) {
            const apiKey = localStorage.getItem('whisperApiKey');
            if (!apiKey) {
                throw new Error('APIキーが設定されていません');
            }

            const formData = new FormData();
            formData.append('file', audioBlob, 'audio.wav');
            formData.append('model', 'whisper-1');

            const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${apiKey}`
                },
                body: formData
            });

            if (!response.ok) {
                throw new Error(`APIリクエストに失敗しました: ${response.statusText}`);
            }

            const result = await response.json();
            return result.text;
        }

        function saveToFile() {
            const text = transcriptionDiv.textContent;
            const blob = new Blob([text], { type: 'text/plain' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'transcription.txt';
            a.click();
            URL.revokeObjectURL(url);
        }

        function copyToClipboard() {
            const text = transcriptionDiv.textContent;
            navigator.clipboard.writeText(text).then(() => {
                statusDiv.textContent = '文字起こしがクリップボードにコピーされました';
            }).catch(err => {
                console.error('クリップボードへのコピーに失敗しました:', err);
                statusDiv.textContent = 'クリップボードへのコピーに失敗しました';
            });
        }
    </script>
</body>
</html>
